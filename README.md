# LSTKD
 Our focus is on knowledge distillation, a method that aims to create smaller yet efficient models. In this research, we specifically explore transferring knowledge from powerful vision transformers to convolutional neural networks (CNNs), like ResNet variants.
